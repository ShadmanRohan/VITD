model:
  name: "banglabert"
  hidden_size: 768
  num_attention_heads: 12
  num_hidden_layers: 12

training:
  batch_size: 32
  learning_rate: 2e-5
  num_epochs: 10
  warmup_steps: 500 